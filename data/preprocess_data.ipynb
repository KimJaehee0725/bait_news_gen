{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from glob import glob\n",
    "\n",
    "data_path = \"original_data/Part1\"\n",
    "CATEGORY_LIST = [\"EC\", \"ET\", \"GB\", \"IS\", \"LC\", \"PO\", \"SO\"]\n",
    "\n",
    "### 1. News 경로를 이용하여 모든 split/카테고리별 경로를 저장\n",
    "News_total_file_dict = {}\n",
    "for split in ['train', 'validation', 'test'] :\n",
    "    news_path_list = glob(f\"{data_path}/{split}/NonClickbait_Auto/*/*\")\n",
    "    News_total_file_dict[split] = {}\n",
    "    for category in CATEGORY_LIST :\n",
    "        news_category_path_list = [path for path in news_path_list if category in path]\n",
    "        News_total_file_dict[split][category] = news_category_path_list\n",
    "\n",
    "### 2. Auto 경로를 이용하여 모든 split/카테고리별 경로를 저장\n",
    "Auto_total_file_dict = {}\n",
    "for split in ['train', 'validation', 'test'] :\n",
    "    auto_path_list = glob(f\"{data_path}/{split}/Clickbait_Auto/*/*\")\n",
    "    Auto_total_file_dict[split] = {}\n",
    "    for category in CATEGORY_LIST :\n",
    "        auto_category_path_list = [path for path in auto_path_list if category in path]\n",
    "        Auto_total_file_dict[split][category] = auto_category_path_list\n",
    "\n",
    "# ### 3. News와 Auto의 파일이름을 비교하여, 다른 이름을 가진 파일의 갯수 확인 -> 중복이 없는 것을 확인\n",
    "# for split in ['train', 'validation', 'test'] :\n",
    "#     news_total = []\n",
    "#     auto_total = []\n",
    "#     for category in CATEGORY_LIST :\n",
    "#         news_file_list = News_total_file_dict[split][category]\n",
    "#         auto_file_list = Auto_total_file_dict[split][category]\n",
    "#         # print(f\"split : {split}, category : {category}, overlap : {len(set(news_file_list) & set(auto_file_list))}\")\n",
    "#         news_total += news_file_list\n",
    "#         auto_total += auto_file_list\n",
    "\n",
    "#     print(f\">>> dataset size | split : {split}, news : {len(news_total)}, auto : {len(auto_total)}, total : {len(news_total) + len(auto_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "total_file_list = []\n",
    "for cate, file_path_list in News_total_file_dict['train'].items() :\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['file_name'] = file_path_list\n",
    "    tmp_dict['category'] = [cate] * len(file_path_list)\n",
    "    total_file_list.append(pd.DataFrame(tmp_dict))\n",
    "\n",
    "for cate, file_path_list in Auto_total_file_dict['train'].items() :\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['file_name'] = file_path_list\n",
    "    tmp_dict['category'] = [cate] * len(file_path_list)\n",
    "    total_file_list.append(pd.DataFrame(tmp_dict))\n",
    "\n",
    "for cate, file_path_list in News_total_file_dict['validation'].items() :\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['file_name'] = file_path_list\n",
    "    tmp_dict['category'] = [cate] * len(file_path_list)\n",
    "    total_file_list.append(pd.DataFrame(tmp_dict))\n",
    "\n",
    "for cate, file_path_list in Auto_total_file_dict['validation'].items() :\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['file_name'] = file_path_list\n",
    "    tmp_dict['category'] = [cate] * len(file_path_list)\n",
    "    total_file_list.append(pd.DataFrame(tmp_dict))\n",
    "\n",
    "for cate, file_path_list in News_total_file_dict['test'].items() :\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['file_name'] = file_path_list\n",
    "    tmp_dict['category'] = [cate] * len(file_path_list)\n",
    "    total_file_list.append(pd.DataFrame(tmp_dict))\n",
    "\n",
    "for cate, file_path_list in Auto_total_file_dict['test'].items() :\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['file_name'] = file_path_list\n",
    "    tmp_dict['category'] = [cate] * len(file_path_list)\n",
    "    total_file_list.append(pd.DataFrame(tmp_dict))\n",
    "\n",
    "total_file_df = pd.concat(total_file_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train T5 데이터 Category 분포 : \n",
      "SO    0.19932\n",
      "GB    0.15052\n",
      "EC    0.13946\n",
      "PO    0.13848\n",
      "IS    0.13842\n",
      "ET    0.12174\n",
      "LC    0.11206\n",
      "Name: category, dtype: float64\n",
      "\n",
      ">> Train News 데이터 Category 분포 : \n",
      "SO    0.199325\n",
      "GB    0.150525\n",
      "EC    0.139475\n",
      "PO    0.138475\n",
      "IS    0.138400\n",
      "ET    0.121750\n",
      "LC    0.112050\n",
      "Name: category, dtype: float64\n",
      "\n",
      ">> Validation News 데이터 Category 분포 : \n",
      "SO    0.199308\n",
      "GB    0.150538\n",
      "EC    0.139462\n",
      "PO    0.138462\n",
      "IS    0.138385\n",
      "ET    0.121769\n",
      "LC    0.112077\n",
      "Name: category, dtype: float64\n",
      "\n",
      ">> Test News 데이터 Category 분포 : \n",
      "SO    0.199308\n",
      "GB    0.150538\n",
      "EC    0.139462\n",
      "PO    0.138462\n",
      "IS    0.138385\n",
      "ET    0.121769\n",
      "LC    0.112077\n",
      "Name: category, dtype: float64\n",
      "\n",
      ">> Fake 데이터 Category 분포 : \n",
      "SO    0.199326\n",
      "GB    0.150523\n",
      "EC    0.139462\n",
      "PO    0.138485\n",
      "IS    0.138417\n",
      "ET    0.121742\n",
      "LC    0.112045\n",
      "Name: category, dtype: float64\n",
      "\n",
      ">> 남은 데이터 수 : 66201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "### 2. News 데이터에서 다음 조건으로 데이터 분리 실행, 이때 category를 이용하여 층화추출 실시 \n",
    "### 2-1. Train :        News - 40_000개, Auto - 40_000개\n",
    "### 2-2. Validation :   News -  13_253개, Auto -  13_253개 -> News가 더 많으므로 Sampling 실시\n",
    "### 2-2. Test :         News -  13_251개, Auto - 13_251개 -> News가 더 많으므로 Sampling 실시\n",
    "\n",
    "def sampling_data(file_df, sample_size = None) :\n",
    "    sampler = StratifiedShuffleSplit(n_splits=1, test_size=sample_size, random_state=42)\n",
    "    sampler = sampler.split(file_df, file_df['category'])\n",
    "    dropped_idx, selected = next(sampler)\n",
    "    selected_df = file_df.iloc[selected].reset_index(drop=True)\n",
    "    dropped_df = file_df.iloc[dropped_idx].reset_index(drop=True)\n",
    "    return selected_df, dropped_df\n",
    "    \n",
    "## 2-1. T5 학습용 데이터(50_000개)\n",
    "train_t5_df, others_df = sampling_data(total_file_df, 50_000)\n",
    "train_t5_df.to_csv('original_data/Part1/sampling_log/train_t5_df.csv')\n",
    "\n",
    "## 2-2. news 데이터(train: 40_000개, val : 13_000개, test : 13_000개)\n",
    "train_news_df, others_df = sampling_data(others_df, 40_000)\n",
    "val_news_df, others_df = sampling_data(others_df, 13_000)\n",
    "test_news_df, others_df = sampling_data(others_df, 13_000)\n",
    "\n",
    "train_news_df.to_csv(\"original_data/Part1/sampling_log/train_news_df.csv\", index=False)\n",
    "val_news_df.to_csv(\"original_data/Part1/sampling_log/val_news_df.csv\", index=False)\n",
    "test_news_df.to_csv(\"original_data/Part1/sampling_log/test_news_df.csv\", index=False)\n",
    "\n",
    "## 2-3. fake 뉴스 생성용 데이터(132_000 개)\n",
    "fake_df, others_df = sampling_data(others_df, 132_000)\n",
    "fake_df.to_csv(\"original_data/Part1/sampling_log/fake_df.csv\", index=False)\n",
    "\n",
    "print(f\">> Train T5 데이터 Category 분포 : \\n{train_t5_df['category'].value_counts(normalize=True)}\\n\")\n",
    "print(f\">> Train News 데이터 Category 분포 : \\n{train_news_df['category'].value_counts(normalize=True)}\\n\")\n",
    "print(f\">> Validation News 데이터 Category 분포 : \\n{val_news_df['category'].value_counts(normalize=True)}\\n\")\n",
    "print(f\">> Test News 데이터 Category 분포 : \\n{test_news_df['category'].value_counts(normalize=True)}\\n\")\n",
    "print(f\">> Fake 데이터 Category 분포 : \\n{fake_df['category'].value_counts(normalize=True)}\\n\")\n",
    "\n",
    "\n",
    "print(f\">> 남은 데이터 수 : {len(others_df)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train T5 데이터 개수 : 50000\n",
      ">> Train News 데이터 개수 : 40000\n",
      ">> Validation News 데이터 개수 : 13000\n",
      ">> Test News 데이터 개수 : 13000\n",
      ">> Fake 데이터 개수 : 132000\n"
     ]
    }
   ],
   "source": [
    "train_t5_path_list = train_t5_df['file_name'].tolist()\n",
    "train_news_path_list = train_news_df['file_name'].tolist()\n",
    "val_news_path_list = val_news_df['file_name'].tolist()\n",
    "test_news_path_list = test_news_df['file_name'].tolist()\n",
    "fake_path_list = fake_df['file_name'].tolist()\n",
    "\n",
    "print(f\">> Train T5 데이터 개수 : {len(train_t5_path_list)}\")\n",
    "print(f\">> Train News 데이터 개수 : {len(train_news_path_list)}\")\n",
    "print(f\">> Validation News 데이터 개수 : {len(val_news_path_list)}\")\n",
    "print(f\">> Test News 데이터 개수 : {len(test_news_path_list)}\")\n",
    "print(f\">> Fake 데이터 개수 : {len(fake_path_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. 데이터셋 구성\n",
    "import json\n",
    "def preprocess(data_path, label = \"news\") :\n",
    "    data_info = {}\n",
    "    json_data = json.load(open(data_path, 'r', encoding='utf-8'))\n",
    "    data_info['news_id'] = json_data['sourceDataInfo']['newsID']\n",
    "    data_info['original_title'] = json_data['sourceDataInfo']['newsTitle']\n",
    "    data_info['content'] = json_data['sourceDataInfo']['newsContent']\n",
    "    data_info['sim_news_id'] = \"\"\n",
    "    data_info['bait_title'] = \"\"\n",
    "    data_info['category'] = data_path.split(\"/\")[-2]\n",
    "    data_info['label'] = 1 if label == \"news\" else 0\n",
    "    return data_info\n",
    "\n",
    "## 4-1. T5 학습용 데이터\n",
    "train_t5_dataset = [preprocess(file_path, label = \"news\") for file_path in train_t5_path_list]\n",
    "\n",
    "## 4-2. news 데이터\n",
    "train_news_dataset = [preprocess(file_path, label = \"news\") for file_path in train_news_path_list]\n",
    "val_news_dataset = [preprocess(file_path, label = \"news\") for file_path in val_news_path_list]\n",
    "test_news_dataset = [preprocess(file_path, label = \"news\") for file_path in test_news_path_list]\n",
    "\n",
    "## 4-3. fake 뉴스 생성용 데이터\n",
    "fake_dataset = [preprocess(file_path, label = \"fake\") for file_path in fake_path_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. 데이터셋 저장\n",
    "import pandas as pd\n",
    "## 5-1. T5 학습용 데이터\n",
    "train_t5_df = pd.DataFrame(train_t5_dataset)\n",
    "train_t5_df.to_csv(\"data/train_t5.csv\", index=False)\n",
    "\n",
    "## 5-2. news 데이터\n",
    "train_news_df = pd.DataFrame(train_news_dataset)\n",
    "val_news_df = pd.DataFrame(val_news_dataset)\n",
    "test_news_df = pd.DataFrame(test_news_dataset)\n",
    "\n",
    "train_news_df.to_csv(\"data/news/train_news.csv\", index=False)\n",
    "val_news_df.to_csv(\"data/news/val_news.csv\", index=False)\n",
    "test_news_df.to_csv(\"data/news/test_news.csv\", index=False)\n",
    "\n",
    "## 5-3. fake 뉴스 생성용 데이터\n",
    "fake_df = pd.DataFrame(fake_dataset)\n",
    "fake_df.to_csv(\"data/fake.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>original_title</th>\n",
       "      <th>content</th>\n",
       "      <th>sim_news_id</th>\n",
       "      <th>bait_title</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IS_M14_322141</td>\n",
       "      <td>한국지엠 창원공장, 생산차 판매량 4년새 40% 급감</td>\n",
       "      <td>한국지엠 군산공장이 지난달 폐쇄한 가운데, 창원공장의 생산 모델이 극심한 판매 부진...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>IS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GB_M12_319028</td>\n",
       "      <td>진공·열압폭탄, 집속탄까지 등장…궁지 몰린 푸틴의 강압 전술</td>\n",
       "      <td>블라디미르 푸틴 러시아 대통령이 우크라이나 침공 방식을 도시 점령을 위한 ‘포위 전...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ET_M13_272058</td>\n",
       "      <td>블랙핑크, COP26 홍보대사 소감 “항상 감사하다”</td>\n",
       "      <td>블랙핑크가 COP26(제 26차 유엔기후변화협약 당사국총회) 홍보대사로 활동하며 긍...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ET</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PO_M08_386870</td>\n",
       "      <td>尹대통령 \\\"日 후쿠시마 원전 오염수 방류, 주변국 동의 받아야\\\"</td>\n",
       "      <td>윤석열 대통령은 26일 일본 정부가 후쿠시마 원전 오염수 해양방류 계획을 인가한 것...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>PO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SO_M06_433109</td>\n",
       "      <td>울산지검, 사회복무요원 확진에 소환 일정 전면 취소</td>\n",
       "      <td>울산지검에 근무하는 사회복무요원이 코로나19 양성 판정을 받아 모든 소환 일정이 취...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         news_id                         original_title  \\\n",
       "0  IS_M14_322141          한국지엠 창원공장, 생산차 판매량 4년새 40% 급감   \n",
       "1  GB_M12_319028      진공·열압폭탄, 집속탄까지 등장…궁지 몰린 푸틴의 강압 전술   \n",
       "2  ET_M13_272058          블랙핑크, COP26 홍보대사 소감 “항상 감사하다”   \n",
       "3  PO_M08_386870  尹대통령 \\\"日 후쿠시마 원전 오염수 방류, 주변국 동의 받아야\\\"   \n",
       "4  SO_M06_433109           울산지검, 사회복무요원 확진에 소환 일정 전면 취소   \n",
       "\n",
       "                                             content sim_news_id bait_title  \\\n",
       "0  한국지엠 군산공장이 지난달 폐쇄한 가운데, 창원공장의 생산 모델이 극심한 판매 부진...                          \n",
       "1  블라디미르 푸틴 러시아 대통령이 우크라이나 침공 방식을 도시 점령을 위한 ‘포위 전...                          \n",
       "2  블랙핑크가 COP26(제 26차 유엔기후변화협약 당사국총회) 홍보대사로 활동하며 긍...                          \n",
       "3  윤석열 대통령은 26일 일본 정부가 후쿠시마 원전 오염수 해양방류 계획을 인가한 것...                          \n",
       "4  울산지검에 근무하는 사회복무요원이 코로나19 양성 판정을 받아 모든 소환 일정이 취...                          \n",
       "\n",
       "  category  label  \n",
       "0       IS      1  \n",
       "1       GB      1  \n",
       "2       ET      1  \n",
       "3       PO      1  \n",
       "4       SO      1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t5_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
